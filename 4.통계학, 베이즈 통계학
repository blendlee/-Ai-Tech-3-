

[통계학]

모수: 평균,분산과 같은 통계를 설명해주는 키워드이다. 평균,분산 뿐만이니라 어떠한 통계를 대표할 수 있는 수이다.

확률분포에는 베르누이분포,카테고리부높,베타분포,감마분포,정규분포,라플라스분포,로그정규 분포등 다향한 확률분포가 존재한다.

우리는 인공지능에서 정확한 통계나 모평균,모분산등을 모르기 때문에 주어진 표본 즉, 측정값으로 모수를 구해야한다.

1. 최대가능도 추정법
이때 필요한 방법은 최대가능도 추정법이다.
최대 가능도 추정법은 확률분포를 모수에 대한 함수로 표현한다음,
미분하여 최적의 모수를 찾는 것이다.

argmax L(theta;x) = argmax P(x|theta)
L(theta;X)는 P(Xi|theta)를 모두 곱한 값이기 때문에 로그를 취해주어 값의 크기를 최적화 한다.

우리는 확률분포를 가정하여 P(x)에 대한 식을 세우고, 위에 식에 P(x)를 대입하여 미분해준 다음, 특정 모수를 구해야한다.

가능도함수는 확률분포함수와 성질이 매우 비슷하기 때문에 가능도함수=확률분포함수 라고 생각하면 편하다!!

2.쿨백-라이블러 발산
쿨백-라이블러 발산 : 데이터공간에 두 개의 확률분포가 존재할 경우 두 확률 분포사이의 거리를 계산하는 함수이다.
정확한 통계 P 와 측정값의 통계 Q 를 쿨백-라이블러 발산을 이용하여 두 통계사이의 거리를 줄이는 것은
결국 실제 통계에 대한 측정 통계의 오차를 줄이는 것이기 때문에 최대가능도 추정법과 동일한 결론이나온다.

KL(P||Q)=sum(P(X)log(P(x)/Q(x)))
        =-E(x~P(x))[log(Q(x)]+E(x~P(x))[log(P(x))]

위 식에서 E(x~P(x))[log(Q(x)]는 크로스 엔트로피라고 하며 오차라고 생각하면 된다.
이를 최소화 한것이 최대가능도 추정법이랑 같다



[베이즈 통계학]

조건부 확률 : 특정한 조건일 때의 사건이 일어날 확률
P(A|B)는 B라는 상황에서 A가 일어날 확률이다.
P(A|B) = P(A∩B)/P(B) 

조건부 확률 키워드!!
True Positive  P(θ)*P(D|θ)            > 실제 결과 : 감염  , 검진결과 : 양성
False Positive P(ㄱθ)*P(D|ㄱθ)        > 실제 결과 : 비감염, 검진결과 : 양성
True Negative  P(ㄱθ)*P(ㄱD|ㄱθ)      > 실제 결과 : 비감염, 검진결과 : 음성
False Negative P(θ)*P(ㄱD|θ)          > 실제 결과 : 감염,   검진결과 : 음성



베이즈 정리는 계속해서 추가되는 데이터들을 활용하여 확률을 업데이트하는 정리이다.

사후확률 = P(θ|D)
사전확률 = P(θ)
가능도 = P(D|θ)
데이터 = P(D)

사후확률 = 사전확률*(가능도/데이터) 로 표현할 수 있다.

예를 들어 코로나의 발병률이 10%이고, 실제로 감염되었을때 검진확률이 99%, 실제로 감염되지 않았을때 검진확률이 1%라고 하자
실제로 결렸을 경우는 θ이며, 검진될 확률을 D이다.

이때 사전확률P(θ) 은 10%, 가능도는 P(D|θ)=99%, P(D|ㄱθ)=1% 이다
P(D)는 P(D|θ)*P(θ)+ P(D|ㄱθ)*P(θ) 이며, 각각의 확률을 대입하여 구할 수 있다.
따라서 우리는 사후확률을 구하기 위한 모든 값들을 구할 수 있다.

구한 사후확률은 추후에 데이터로 갱신이되며 이후 사후확률을 구할때 전에 구한 사후확률을 사전확률에 대입하여준다.

조건부 확률 주의!!!!
조건부 확률은 인과관계를 추론할 때 사용 금지!!!!!

인과관계를 추론할 때 가장 주의할 점은 중첩효과를 제거해야 한다는 것이다.
중첩효과는 인과관계에 대한 다른 요인이 존재할 수 있기 때문에, 중첩되는 모든 요인을 제거하고 인과관계를 추론하여야 정확한 추론이 가능하다.


---------가능도함수와 최대 가능도 추정법 코드 --------------

import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
from scipy.stats import norm

plt.figure(figsize=(10,6))

x = np.linspace(-5, 5, 100)

p1 = sp.stats.norm(loc=-1).pdf(1)
p2 = sp.stats.norm(loc=0).pdf(1)
p3 = sp.stats.norm(loc=1).pdf(1)

plt.scatter(1, p1, s=100, c='blue', marker='v', 
         label=r"$N(x_1;\mu=-1)$={:.2f}".format(np.round(p1, 2)))
plt.scatter(1, p2, s=100, c='orange', marker='^', 
         label=r"$N(x_1;\mu=0)$={:.2f}".format(np.round(p2, 2)))
plt.scatter(1, p3, s=100, c='green', marker='s', 
         label=r"$N(x_1;\mu=1)$={:.2f}".format(np.round(p3, 2)))


plt.plot(x,(2*np.pi)**(-1/2)*np.e**((-(x+1)**2)/2) , ls="-.")
plt.plot(x,(2*np.pi)**(-1/2)*np.e**((-(x)**2)/2) , ls="--")
plt.plot(x,(2*np.pi)**(-1/2)*np.e**((-(x-1)**2)/2) , ls="-")


plt.scatter(1, 0, s=100, c='k')
plt.vlines(1, -0.09, 0.45, linestyle=":")
plt.hlines(0, -5, 5, colors='gray', linestyle="-")
plt.text(1-0.3, -0.15, "$x_0=1$")

plt.xlabel("x")
plt.ylabel("likelihood")
plt.legend()
plt.title("MLE for population mean")
plt.show()

x0=1


print('mu=-1: likelihood at x_0=1 is {:.4f}'.format(1/np.sqrt(2*np.pi)*np.exp(-2)))
print('mu=0: likelihood at x_0=1 is {:.4f}'.format(1/np.sqrt(2*np.pi)*np.exp(-1/2)))
print('mu=1: likelihood at x_0=1 is {:.4f}'.format(1/np.sqrt(2*np.pi)*np.exp(0)))


