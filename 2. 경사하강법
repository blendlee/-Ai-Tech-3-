
[경사하강법]



경사하강법이란 쉽게 말해서 어떠한 함수의 극대 or 극소점을 구하는 방법 중 하나이다.

기본적인 원리는 특정한 점에서 미분값을 구하고 이 값을 x값에서 빼면
x좌표는 극소점으로 수렴하게 되어있다.

----소스코드----

var=init
grad=gradient(var)

#극소점을 구하기 위해선 기울기가 0인 지점을 구해야하지만 정확히 0인지점을 찾을 수 없기에 0에 가까운 엡실론 값에 근사한다.
while grad > epsilon :
  
  var=var-l*grad
  #여기서 l은 학습률인데, 학습률에 따라 극소점을 찾는 보폭(step)을 조절하는 것이다.
  #이를 너무 크게해서도 안되고, 작게 해서도 안된다고 생각한다.
  
  grad=gardient(var)

----------------

sympy로 구현하기 위한 메소드 소드모음

import sympy as sym
from sympy.abc import x,y

sym.poly(function) >> 함수로 정의
sym.diff(function,x) >>미분
fuction.subs(x,val) >>대입


-----------------

하지만 딥러닝에서는 변수가 x가 아닌 여러개의 벡터 성분으로 정의 되므로 우리는 다차원에서 경사하강법을 진행할 필요가 있다.
이를 위해선 편미분을 이용한다.
편미분을 이용하여 "Gradient Vector"를 구하였다면 이에 -1을 곱하여야 한다.
-1을 곱하는 이유는 gradient vector의 방향성을 위해서이다. 
특정한 점,면으로 부터 볼록한 평면의 극소점으로 향하는 벡터여야 우리가 연산을 쉽게할 수 있기 때문이다.
(-1을 곱하기 전에는 함수평면으로 부터 특정한 점으로 향하는 gradient vector이다)
이의 소스코드는 위에서의 경사하강법 코드에  grad 대신 norm(grad)로 바꿔주면 된다.
(norm(grad)가 최소일때 극소점이기 때문)





[경사하강법 심화]

실질적으로 경사하강법을 딥러닝에 쓰기 위해서는 단순한 경사하강법으로는 불가능 하다.
원초적으로 우리는 선형회귀분석을 통하여 컴퓨터를 학습시켜야하는데,
이러한 선형회귀분석을 경사하강법에 적용시켜야 우리가 원하는 학습을 시킬 수 있다.

성형회귀분석에서 우리는 y-y'이 최소가 되도록 만드는 beta를 구하여야 했다.
이를 경사하강법에 적용시킨다면

X*b=y^ 에서 beta로 미분하여 경사하강법을 진행하면 된다.
y=f(x)를 경사하강법 한다고 했을때 y는 ||y^-X*b||로 x는 b로 바뀐 것일 뿐이다.

하지만 여기서 어려웠던 점은 X,b,y 가 전부 벡터,행렬이라는 것이다

우리는 여러가지 다양한 beta에 대하여 ||y^-X*b||의 미분을 진행하여야 한다.
(편의에 따라서 ||y^-X*b||의 제곱을 해주어도 상관없다)

손으로 직접 미분해본 결과
-X(k열의 전치행렬)(y-X*b)/(n||y-X*b||)
가 나왔다.

이제 위의 식을 이용하여 경사하강법을 진행해주면 된다.
b2=b1-l*(-X(k열의 전치행렬)(y-X*b)/(n||y-X*b||))

제곱한 결과에 강사하강법을 진행하면
b2=b1-l*(-X(k열의 전치행렬)(y-X*b)) 
으로 꽤 단순하게 나온다.


--------소스코드----------

for i in range(학습횟수):
  error=y-X@b
  grad= -transpose(x)@error
  beta=beta-lr*grad


-------------------------


위의 경사하강법이 적용되는 범위는 언제나 함수가 볼룩함수여야 한다.
볼록함수가 아니라면 선형회귀대신 비선형회귀를 하여야하고, 위와 같은 방식으로 경사하강법을 적용할 경우 수렴하지 않는 경우가 발생한다.

따라서 나온 다른 방법은 확률적 경사하강법이다.

확률적 경사하강법은 위와 같이 경사하강법을 진행하지만, minibatch라는 적당량의 일부 데이터 만으로 경사하강법을 진행하는것이다.
매 계산마다 미니배치를 다르게 잡는다면 방향을 함수의 볼록점에 도달하였을때 탈출할 수 있다는 장점을 가지고 있다.
따라서 미니배치를 다르게 잡아 회귀를 시킨다면 우리가 원하는 극소점으로 회귀를 시킬수 있다.
