



[딥러닝 학습방법에 대한 이해]



저번 TIL까지는 경사하강법을 이용한 '선형회귀'를 공부하였다.
이번에는 전반적인 딥러닝 학습방법의 공부를 하였다.
애초에 모든 데이터셋들이 전부 선형으로 회귀할 확률은 굉장히 높기 때문에 우리는 비선형모델을 통한 학습을 시켜야 한다.

기초에서 배운 선형모델들을 비선형 모델로 전환 시키기 위해선 여러가지의 활성함수를 적용시킨다.
시그모이드, 하이퍼볼릭탄젠트, 렐루 함수가 대표적인 예시이다.
시그모이드는 1/(1+e^-x), 하이퍼볼렉탄젠트는 (e^x+e^-x)/(e^x-e^-x), 렐루는 max(0,x) 연산이 적용된다.

이런식으로 선형모델을 비선형모델로 만들고, 마지막으로 이 결과물들을 확률로 해석해준다면 딥러닝 모델이 완성된다.
이때 확률로 해석해주는 함수는 softmax 연산이라고 한다. softmax는 모든 output들에 대하여 exponential을 취해주고, 그냥 확률을 구해주면된다.
하지만 여기서 바로 exponential을 취해주는 것보다는 max값을 빼준다음에 취해주어야 overflow가 안생긴다.

-Foward Propagation Mechanism
1. input X행렬에 대하여
2. 선형모델을 취해주면 Z= W*X+b라는 결과가 나오게 되고,
3. Z를 위의 활성함수에 대입시켜준다.
4. O=f(z)
5. O를 다시 input

이 5단계의 과정을 하나의 neural network라고 하며, 이를 여러번 반복한다면, multi-layer-perceptron이 된다.


하지만 우리가 모델을 완성하려면 각각 단계에 대한 가중치행렬 W와 b를 구해야한다.
이를 구하는 과정이 컴퓨터를 학습시키는 과정이고, 이를 학습시키기 위해서는 우리는 반대로 접근하여야한다.
이를 Back propagation이라고 하며, 경사하강법을이용하여 모든단계에 대한 가중치행렬을 구한다.
우리는 Loss를 W(i,j)에 대하여 미분하며 경사하강법을 진행한다.

최종 Output행렬 부터 L층의 가중치 행렬까지 구하여야 한다.
이때 사용되는 개념이 바로 체인룰이다. (쉽게 말하면 그냥 합성함수의 미분법 개념)
체인룰을 5번정도 계산해본결과...
나왔따.....

2층 layer에 대하여 Lossfuction을 W(i,j)에 대하여 미분한다면
다음과 같이 나온다

계산하면서 주의해야될 점은, 항상 행과열의 의미를 알아야 한다는점!!!
하나의 데이터에 대한 정보는 열이다!!!
행은 여러데이터들의 n번째 정보이다!!




[확률론]

경사하강법으로 손실함수를 최소화시켜 선형회귀를 한것도 결국 확률론의 일부이다.
loss fuction의 분산을 최소화시키면서 구한것이다.
이는 회귀분석에서 사용하는 방법이고, 분류문제에서는 모델 예측의 불확실성읠 최소화하는 방법으로 구한다.

Dataset D에 대하여 데이터들의 분포를 확률분포로 표현할 수 있다.
이때 P(x,y)라 표기한다.

P(x|y)는 y가 정해져있을때 y값에 따른 x의 확률분포이고,
P(y|x)는 x가 정해져있을때 x가 y일 확률을 의미한다
우리는 Foward Propagation을 진행하고, softmax로 확률로 변환시킨다음, 
분류문제의 경우 이를 이용하여 조건부확률을 계산한다.
반면 회귀문제의 경우 조건부 기대값을 계산하여야 한다.


-몬테카를로 샘플링
쉽게말해서 점을 무수히 많이 찍고, 점이 특정영역에 속할 확률을 측정값으로 얻어 특정영역의 넓이를 근사하는 방법이다.



------소스코드-------
len=start-end
stat=[]
for _ in range(1000):
  x=np.random.uniform(low=end,high=start,size=1000)
  fun_x=func(x)
  val=len*np.mean(fun_x)
  stat.append(val)

answer=np.mean(stat),np.std(stat)


---------------------




개인적으로는 오늘이 제일 머리 터지는 날이었다.
체인룰을 이용한 Loss function 을 weight(1)로 미분하기....
손으로 5,6번정도 해보면서 완벽히 유도가 가능해졌다!!
또한 확률론도 어려운 용어들이 많이 나와서 머리가 터졌지만 결국에 고등학교 확통에서 조금만 용어를 변형시킨것이라
여러번 읽으니까 이해가 되긴하더라...
머리가 너무아프다 ㅎㅎ
